{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ada4ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Counter in c:\\users\\grace\\anaconda3\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\grace\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\grace\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\grace\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\grace\\anaconda3\\lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e7c63d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "\n",
    "import regex as re\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models\n",
    "# from transformers import pipeline\n",
    "import string\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# import liwc\n",
    "import collections\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# from transformers import BertModel\n",
    "# from transformers import AutoTokenizer\n",
    "# from transformers import pipeline\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "save_progress = \"save_progress/\"\n",
    "data = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b186f6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'C:\\\\Users\\\\grace\\\\Desktop\\\\macs_404_patterns\\\\folktales\\\\utils.py'>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfb752",
   "metadata": {},
   "source": [
    "# Processing story data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6ad5967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = pd.read_csv(data+'folk_tales_deduplicated.csv')\n",
    "stories = stories[['nation','title','text']]\n",
    "stories = stories.dropna(subset = ['nation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578202e",
   "metadata": {},
   "source": [
    "### Getting lemmas/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5bdb029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = [utils.to_wordlist([stories.text[x]]) for x in stories.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "37da82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist_bystory = []\n",
    "for story in wordlist:\n",
    "    wordlist_bystory.append([word for chunk in story for word in chunk])\n",
    "stories['wordlist'] = wordlist_bystory\n",
    "stories['cleaned'] = [\" \".join(stories.wordlist[i]).replace(\"-\",\" \") for i in stories.index]\n",
    "stories['cleaned'] = [re.sub('[^a-zA-Z ]','',x) for x in stories.cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "823b678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stories.loc[stories['cleaned'].str.contains(' thor ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cafbd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = stories['cleaned'].apply(utils.get_lemmas)\n",
    "lemmas.to_pickle(save_progress+'story_lemmas.pkl')\n",
    "lemmas = pd.read_pickle(save_progress+'story_lemmas.pkl')\n",
    "stories['lemmas'] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6ae27023",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrased = utils.make_bigrams(lemmas)\n",
    "stories['phrased'] = phrased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd75f18",
   "metadata": {},
   "source": [
    "### Getting language families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1ee1ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data+\"grouped_regions.json\") as f:\n",
    "    grouped_regions = json.load(f)\n",
    "\n",
    "lang_fam = []\n",
    "for i in stories.index:\n",
    "    hasfam = False\n",
    "    for key in grouped_regions.keys():\n",
    "        if stories.loc[i,'nation'] in grouped_regions[key]:\n",
    "            lang_fam.append(key)\n",
    "            hasfam = True\n",
    "    if not hasfam:\n",
    "        lang_fam.append(np.nan)\n",
    "stories['language_family'] = lang_fam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03ff0e",
   "metadata": {},
   "source": [
    "### Saving/reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "64460b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories['sid'] = stories.index\n",
    "stories['sid'] = stories['sid'].apply(lambda x: 'sid'+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "6c918720",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories.to_csv(save_progress+'stories.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "39de3bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>wordlist</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>phrased</th>\n",
       "      <th>language_family</th>\n",
       "      <th>sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japanese</td>\n",
       "      <td>Momotaro</td>\n",
       "      <td>If you’ll believe me there was a time when the...</td>\n",
       "      <td>['believe', 'time', 'fairies', 'none', 'shy', ...</td>\n",
       "      <td>believe time fairies none shy time beasts talk...</td>\n",
       "      <td>['believe', 'time', 'fairy', 'none', 'time', '...</td>\n",
       "      <td>['believe', 'time', 'fairy', 'none', 'time', '...</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>sid0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>serbian</td>\n",
       "      <td>The Birdcatcher</td>\n",
       "      <td>Near Constantinople there lived a man who knew...</td>\n",
       "      <td>['near', 'constantinople', 'lived', 'man', 'kn...</td>\n",
       "      <td>near constantinople lived man knew occupation ...</td>\n",
       "      <td>['near', 'constantinople', 'live', 'knew', 'oc...</td>\n",
       "      <td>['near', 'constantinople', 'live', 'knew', 'oc...</td>\n",
       "      <td>slavic</td>\n",
       "      <td>sid1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     nation            title  \\\n",
       "0  japanese         Momotaro   \n",
       "1   serbian  The Birdcatcher   \n",
       "\n",
       "                                                text  \\\n",
       "0  If you’ll believe me there was a time when the...   \n",
       "1  Near Constantinople there lived a man who knew...   \n",
       "\n",
       "                                            wordlist  \\\n",
       "0  ['believe', 'time', 'fairies', 'none', 'shy', ...   \n",
       "1  ['near', 'constantinople', 'lived', 'man', 'kn...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  believe time fairies none shy time beasts talk...   \n",
       "1  near constantinople lived man knew occupation ...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  ['believe', 'time', 'fairy', 'none', 'time', '...   \n",
       "1  ['near', 'constantinople', 'live', 'knew', 'oc...   \n",
       "\n",
       "                                             phrased language_family   sid  \n",
       "0  ['believe', 'time', 'fairy', 'none', 'time', '...       east_asia  sid0  \n",
       "1  ['near', 'constantinople', 'live', 'knew', 'oc...          slavic  sid1  "
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories = pd.read_csv(save_progress+'stories.csv')\n",
    "sid_nation = {}\n",
    "for i in stories.index:\n",
    "    sid_nation[stories['sid'][i]] = stories['nation'][i]\n",
    "nation_sid = {}\n",
    "for nation in stories.nation.unique():\n",
    "    nation_sid[nation] = list(stories.query('nation == @nation')['sid'])\n",
    "stories.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e41d0",
   "metadata": {},
   "source": [
    "## Processing Hofstede data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "20910d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "hofstede = pd.read_excel(data+'hofstede.xlsx') # https://www.kaggle.com/datasets/aleksakenjic/hofstedes-cultural-dimensions\n",
    "language = pd.read_csv(data+'countries_languages.csv') # https://www.kaggle.com/datasets/shubhamptrivedi/languages-spoken-across-various-nations\n",
    "language['main'] = language['Languages Spoken'].apply(lambda x: x.split(' ')[0].replace(',','').split('/')[0])\n",
    "hofstede = pd.merge(hofstede,language[['Country','main']],how='left',left_on='country',right_on='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b37cded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctr</th>\n",
       "      <th>country</th>\n",
       "      <th>pdi</th>\n",
       "      <th>idv</th>\n",
       "      <th>mas</th>\n",
       "      <th>uai</th>\n",
       "      <th>ltowvs</th>\n",
       "      <th>ivr</th>\n",
       "      <th>estim</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Country</th>\n",
       "      <th>main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BOS</td>\n",
       "      <td>Bosnia</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>87</td>\n",
       "      <td>69.773300</td>\n",
       "      <td>44.196429</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CZE</td>\n",
       "      <td>Czech Rep</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>57</td>\n",
       "      <td>74</td>\n",
       "      <td>70.025189</td>\n",
       "      <td>29.464286</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DOM</td>\n",
       "      <td>Dominican Rep</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>13.098237</td>\n",
       "      <td>54.241071</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>HOK</td>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>68</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>60.957179</td>\n",
       "      <td>16.964286</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>KOR</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>85</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>29.464286</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>MAC</td>\n",
       "      <td>North Macedonia</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>87</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>PUE</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>56</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.955357</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>STP</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "      <td>75</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>70</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>SLK</td>\n",
       "      <td>Slovak Rep</td>\n",
       "      <td>100</td>\n",
       "      <td>52</td>\n",
       "      <td>110</td>\n",
       "      <td>51</td>\n",
       "      <td>76.574307</td>\n",
       "      <td>28.348214</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>USA</td>\n",
       "      <td>U.S.A.</td>\n",
       "      <td>40</td>\n",
       "      <td>91</td>\n",
       "      <td>62</td>\n",
       "      <td>46</td>\n",
       "      <td>25.692695</td>\n",
       "      <td>68.080357</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ctr                country  pdi  idv  mas  uai      ltowvs        ivr  \\\n",
       "11  BOS                 Bosnia   90   22   48   87   69.773300  44.196429   \n",
       "21  CZE              Czech Rep   57   58   57   74   70.025189  29.464286   \n",
       "23  DOM          Dominican Rep   65   30   65   45   13.098237  54.241071   \n",
       "34  HOK              Hong Kong   68   25   57   29   60.957179  16.964286   \n",
       "46  KOR            South Korea   60   18   39   85  100.000000  29.464286   \n",
       "62  MAC        North Macedonia   90   22   45   87   62.000000  35.000000   \n",
       "70  PUE            Puerto Rico   68   27   56   38    0.000000  89.955357   \n",
       "73  STP  Sao Tome and Principe   75   37   24   70   32.000000  41.000000   \n",
       "77  SLK             Slovak Rep  100   52  110   51   76.574307  28.348214   \n",
       "89  USA                 U.S.A.   40   91   62   46   25.692695  68.080357   \n",
       "\n",
       "    estim  Unnamed: 9 Unnamed: 10 Country main  \n",
       "11      1         NaN         NaN     NaN  NaN  \n",
       "21      0         NaN         NaN     NaN  NaN  \n",
       "23      1         NaN         NaN     NaN  NaN  \n",
       "34      0         NaN         NaN     NaN  NaN  \n",
       "46      0         NaN         NaN     NaN  NaN  \n",
       "62      1         NaN         NaN     NaN  NaN  \n",
       "70      1         NaN         NaN     NaN  NaN  \n",
       "73      1         NaN         NaN     NaN  NaN  \n",
       "77      0         NaN         NaN     NaN  NaN  \n",
       "89      0         NaN         NaN     NaN  NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hofstede[hofstede['main'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "7d5ad6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hofstede_fillna = {'Bosnia':'bosnian','Czech Rep':'czechoslovak','South Korea':'korean','U.S.A':'English'}\n",
    "hofstede['main'] = hofstede['main'].apply(lambda x: hofstede_fillna[x] if x in hofstede_fillna.keys() else x)\n",
    "hofstede = hofstede.groupby('main').mean().reset_index()\n",
    "hofstede['main'] = hofstede['main'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "3d4f26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_hofstede = pd.merge(stories[['sid','nation','language_family','title','text']], hofstede[['pdi','idv','mas','uai','ltowvs','ivr','main']], left_on='nation', right_on='main', how='inner').drop(columns='main')\n",
    "stories_hofstede.to_csv(save_progress+'stories_hofstede.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "57b5f9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>nation</th>\n",
       "      <th>language_family</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pdi</th>\n",
       "      <th>idv</th>\n",
       "      <th>mas</th>\n",
       "      <th>uai</th>\n",
       "      <th>ltowvs</th>\n",
       "      <th>ivr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sid0</td>\n",
       "      <td>japanese</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>Momotaro</td>\n",
       "      <td>If you’ll believe me there was a time when the...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.90932</td>\n",
       "      <td>41.741071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sid65</td>\n",
       "      <td>japanese</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>The Filial Girl</td>\n",
       "      <td>A girl once lived in the province of Echigo, w...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.90932</td>\n",
       "      <td>41.741071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sid109</td>\n",
       "      <td>japanese</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>The Tongue-Cut Sparrow</td>\n",
       "      <td>Once upon a time there was an old man who live...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.90932</td>\n",
       "      <td>41.741071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sid122</td>\n",
       "      <td>japanese</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>The Mallet</td>\n",
       "      <td>There were once two farmer men who were brothe...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.90932</td>\n",
       "      <td>41.741071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sid210</td>\n",
       "      <td>japanese</td>\n",
       "      <td>east_asia</td>\n",
       "      <td>Karma</td>\n",
       "      <td>The young man, Ito Tatewaki, was returning hom...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.90932</td>\n",
       "      <td>41.741071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sid    nation language_family                   title  \\\n",
       "0    sid0  japanese       east_asia                Momotaro   \n",
       "1   sid65  japanese       east_asia         The Filial Girl   \n",
       "2  sid109  japanese       east_asia  The Tongue-Cut Sparrow   \n",
       "3  sid122  japanese       east_asia              The Mallet   \n",
       "4  sid210  japanese       east_asia                   Karma   \n",
       "\n",
       "                                                text   pdi   idv   mas   uai  \\\n",
       "0  If you’ll believe me there was a time when the...  54.0  46.0  95.0  92.0   \n",
       "1  A girl once lived in the province of Echigo, w...  54.0  46.0  95.0  92.0   \n",
       "2  Once upon a time there was an old man who live...  54.0  46.0  95.0  92.0   \n",
       "3  There were once two farmer men who were brothe...  54.0  46.0  95.0  92.0   \n",
       "4  The young man, Ito Tatewaki, was returning hom...  54.0  46.0  95.0  92.0   \n",
       "\n",
       "     ltowvs        ivr  \n",
       "0  87.90932  41.741071  \n",
       "1  87.90932  41.741071  \n",
       "2  87.90932  41.741071  \n",
       "3  87.90932  41.741071  \n",
       "4  87.90932  41.741071  "
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_hofstede = pd.read_csv(save_progress+'stories_hofstede.csv')\n",
    "stories_hofstede.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc3c6f",
   "metadata": {},
   "source": [
    "# Lemma Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a200da7",
   "metadata": {},
   "source": [
    "## % of stories that mention x lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "d47c61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_lemmacount = pd.DataFrame()\n",
    "for i in stories.index:\n",
    "    lemma_count = pd.DataFrame(pd.Series(stories.iloc[i]['phrased'].strip(\"[]\").replace('\\'',\"\").split(\", \")).value_counts())\n",
    "    lemma_count.columns = ['sid'+str(i)]\n",
    "    stories_lemmacount = pd.merge(stories_lemmacount,lemma_count,left_index=True,right_index=True,how=\"outer\")\n",
    "stories_lemmacount = stories_lemmacount.fillna(0)\n",
    "\n",
    "stories_lemmacount = stories_lemmacount.reset_index()\n",
    "stories_lemmacount = stories_lemmacount.rename(columns={'index':'lemma'})\n",
    "stories_lemmacount = stories_lemmacount.set_index('lemma')\n",
    "stories_lemmacount.to_csv(save_progress+\"stories_lemmacount.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9820f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_lemmacount = pd.read_csv(save_progress+\"stories_lemmacount.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "96672071",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_lemmacountT = stories_lemmacount.copy().T.reset_index()\n",
    "stories_lemmacountT = stories_lemmacountT.rename(columns={'level_0':'sid'})\n",
    "stories_lemmacountT = stories_lemmacountT.set_index('sid')\n",
    "stories_lemmacountT = pd.merge(stories_lemmacountT,stories[['sid','nation','language_family']],left_index=True,right_on='sid')\n",
    "stories_lemmacountT = stories_lemmacountT.rename(columns = {'nation_y':'nation'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3e793c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sid0', 'sid2184', 'sid2577'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_lemmacount.columns[np.where(stories_lemmacount.loc['momotaro']!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c7389629",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_storycount = dict(stories.groupby('nation').count()['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "08350e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemmamentions = stories_lemmacountT.groupby('nation').sum().T.rename_axis('lemma')\n",
    "for nation in nation_lemmamentions.columns:\n",
    "    nation_lemmamentions[nation] = nation_lemmamentions[nation].apply(lambda x: x/nation_storycount[nation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8fd662f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = pd.read_csv(data+'bigrams.csv').set_index('bigram').rename(columns={' count':'count'})\n",
    "bigrams['count'] = bigrams['count'] + 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b31fdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_englishness = []\n",
    "for lemma in nation_lemmafreq.index:\n",
    "    lemma_englishness.append(utils.bigram_log_f(bigrams,lemma)/len(lemma))\n",
    "nation_lemmamentions['englishness'] = lemma_englishness\n",
    "\n",
    "nation_lemmamentions['common'] = nation_lemmamentions[stories.nation.unique()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8bea9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemmamention.to_csv(save_progress+'nation_lemmamentions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e9d5aa8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albanian</th>\n",
       "      <th>arabic</th>\n",
       "      <th>armenian</th>\n",
       "      <th>australian_ethnic</th>\n",
       "      <th>belgian</th>\n",
       "      <th>brazilian</th>\n",
       "      <th>bukovinian</th>\n",
       "      <th>bulgarian</th>\n",
       "      <th>canadian_native</th>\n",
       "      <th>cataloanian</th>\n",
       "      <th>...</th>\n",
       "      <th>south_african</th>\n",
       "      <th>spanish</th>\n",
       "      <th>swedish</th>\n",
       "      <th>tanzanian</th>\n",
       "      <th>turkish</th>\n",
       "      <th>ukrainian</th>\n",
       "      <th>welsh</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>englishness</th>\n",
       "      <th>common</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.123297</td>\n",
       "      <td>0.020761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>1.5</td>\n",
       "      <td>17.268643</td>\n",
       "      <td>0.568957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         albanian    arabic  armenian  australian_ethnic  belgian  brazilian  \\\n",
       "lemma                                                                          \n",
       "abandon      0.04  0.097561  0.032258           0.000000      0.0   0.000000   \n",
       "able         0.56  0.902439  0.419355           0.408163      0.4   0.225806   \n",
       "\n",
       "         bukovinian  bulgarian  canadian_native  cataloanian  ...  \\\n",
       "lemma                                                         ...   \n",
       "abandon         0.0        0.0         0.076923          0.0  ...   \n",
       "able            1.0        0.5         0.384615          3.0  ...   \n",
       "\n",
       "         south_african   spanish   swedish  tanzanian  turkish  ukrainian  \\\n",
       "lemma                                                                       \n",
       "abandon       0.083333  0.000000  0.129032        0.0   0.1250   0.000000   \n",
       "able          0.333333  0.615385  0.806452        0.5   0.8125   0.148148   \n",
       "\n",
       "            welsh  zimbabwe  englishness    common  \n",
       "lemma                                               \n",
       "abandon  0.000000       0.0    20.123297  0.020761  \n",
       "able     0.388889       1.5    17.268643  0.568957  \n",
       "\n",
       "[2 rows x 59 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_lemmamentions = pd.read_csv(save_progress+'nation_lemmamentions.csv',index_col=0)\n",
    "nation_lemmamentions.query('albanian != 0').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e876b",
   "metadata": {},
   "source": [
    "# percentile of group in lemma usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7fb31",
   "metadata": {},
   "source": [
    "count(lemma) / len(story words) --> avg freq --> percentile of usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bf1a6",
   "metadata": {},
   "source": [
    "\"nation a is in the top n% of users of lemma x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "94161c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_lemmafreq = stories_lemmacount.copy()\n",
    "stories_lemmafreq = stories_lemmafreq.div(stories_lemmacount.sum(axis=1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "24acadb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid0</th>\n",
       "      <th>sid2184</th>\n",
       "      <th>sid2577</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aalborg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarhus</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aasvo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuur</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuya</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuyder</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuyderzee</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zwanzigers</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33763 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sid0  sid2184  sid2577\n",
       "lemma                             \n",
       "aalborg      0.0      0.0      0.0\n",
       "aard         0.0      0.0      0.0\n",
       "aarhus       0.0      0.0      0.0\n",
       "aaron        0.0      0.0      0.0\n",
       "aasvo        0.0      0.0      0.0\n",
       "...          ...      ...      ...\n",
       "zuur         0.0      0.0      0.0\n",
       "zuya         0.0      0.0      0.0\n",
       "zuyder       0.0      0.0      0.0\n",
       "zuyderzee    0.0      0.0      0.0\n",
       "zwanzigers   0.0      0.0      0.0\n",
       "\n",
       "[33763 rows x 3 columns]"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_lemmafreq[stories_lemmafreq.columns[np.where(stories_lemmafreq.loc['momotaro']!=0)[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "6af3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *! groupby\n",
    "groupby = 'nation' # nation, language_family\n",
    "wrt = 'nation' # nation/language_family, lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "e0731663",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemmamean = pd.merge(stories_lemmafreq.T,stories[['sid','nation','language_family']],left_index=True,right_on='sid')\n",
    "nation_lemmamean = nation_lemmamean.rename(columns={'nation_y':'nation'})\n",
    "nation_lemmamean = nation_lemmamean.groupby(groupby).mean()\n",
    "nation_lemmameanT= nation_lemmamean.T\n",
    "\n",
    "nation_lemmamean_save = nation_lemmameanT\n",
    "nation_lemmamean_save['englishness'] = lemma_englishness\n",
    "nation_lemmamean_save['common'] = nation_lemmamentions[stories.nation.unique()].mean(axis=1)\n",
    "nation_lemmamean_save.to_csv(save_progress+'nation_lemmamean.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "26fea9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_percentile = pd.DataFrame()\n",
    "if wrt == 'nation' or wrt == 'language_family':\n",
    "    for col in nation_lemmameanT:\n",
    "        lemma_percentile[col] = [percentileofscore(nation_lemmameanT[col], value) for value in nation_lemmameanT[col]]\n",
    "elif wrt == 'lemma':\n",
    "    for col in nation_lemmamean:\n",
    "        lemma_percentile[col] = [percentileofscore(nation_lemmamean[col], value) for value in nation_lemmamean[col]]\n",
    "    lemma_percentile = lemma_percentile.T\n",
    "lemma_percentile.columns = list(nation_lemmamean.index.values)\n",
    "# lemma_percentile['englishness'] = lemma_englishness\n",
    "# lemma_percentile['common'] = nation_lemmamentions[stories.nation.unique()].mean(axis=1)\n",
    "lemma_percentile = lemma_percentile.rename(index=dict(zip(range(len(list(nation_lemmamean.columns.values))),list(nation_lemmamean.columns.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "cdd7f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemma_lemmapercentile['englishness'] = lemma_englishness\n",
    "nation_lemma_lemmapercentile['common'] = nation_lemmamentions[stories.nation.unique()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "4ac5750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemma_lemmapercentile.to_csv(save_progress+\"nation_lemma_lemmapercentile.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "7c10f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_percentile.to_csv(save_progress+f\"{groupby}_lemma_{wrt}percentile.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "27c12c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_lemma_lemmapercentile = pd.read_csv(save_progress+\"nation_lemma_lemmapercentile.csv\",index_col=0)\n",
    "nation_lemma_nationpercentile = pd.read_csv(save_progress+\"nation_lemma_nationpercentile.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003cad1",
   "metadata": {},
   "source": [
    "uniqueness to group = langfam/nation percentile <br>\n",
    "uniqueness within group = lemma percentile <br>\n",
    "<br>\n",
    "lemma_uniquness = unique1 x unique2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "1fbad35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_unique_nation = pd.DataFrame(index=dict(zip(range(len(list(nation_lemmamean.columns.values))),list(nation_lemmamean.columns.values))))\n",
    "for col in nation_lemma_lemmapercentile.columns:\n",
    "    lemma_unique_nation[col] = nation_lemma_lemmapercentile[col].values+.5*nation_lemma_nationpercentile[col].values\n",
    "lemma_unique_nation = lemma_unique_nation.rename(index=dict(zip(range(len(list(nation_lemmamean.columns.values))),list(nation_lemmamean.columns.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "9a23aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_unique_nation['englishness'] = lemma_englishness\n",
    "lemma_unique_nation['common'] = nation_lemmamentions[stories.nation.unique()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "8f7b4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_unique_nation.to_csv(save_progress+'lemma_unique_nation.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9de2e",
   "metadata": {},
   "source": [
    "### language family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "1b0205b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfam_lemmapercentile = pd.merge(stories_lemmafreq.T,stories[['sid','nation','language_family']],left_index=True,right_on='sid')\n",
    "langfam_lemmapercentile = langfam_lemmapercentile.rename(columns={'nation_y':'nation'})\n",
    "langfam_lemmapercentile = langfam_lemmapercentile.groupby('language_family').mean()\n",
    "for col in langfam_lemmapercentile:\n",
    "    langfam_lemmapercentile[col] = [percentileofscore(langfam_lemmapercentile[col], value) for value in langfam_lemmapercentile[col]]\n",
    "langfam_lemmapercentile = langfam_lemmapercentile.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "23663127",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfam_lemmapercentile['englishness'] = lemma_englishness\n",
    "langfam_lemmapercentile['common'] = langfam_lemmapercentile[stories.language_family.unique()].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "8200121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfam_lemmapercentile.to_csv(save_progress+\"langfam_lemmapercentile.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "611cc5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>albanian</th>\n",
       "      <th>armenic</th>\n",
       "      <th>austronesian</th>\n",
       "      <th>bantu</th>\n",
       "      <th>celtic</th>\n",
       "      <th>chadic</th>\n",
       "      <th>east_asia</th>\n",
       "      <th>germanic</th>\n",
       "      <th>hellenic</th>\n",
       "      <th>hindustani</th>\n",
       "      <th>italic</th>\n",
       "      <th>native_american</th>\n",
       "      <th>native_cana</th>\n",
       "      <th>semitic</th>\n",
       "      <th>slavic</th>\n",
       "      <th>turkic</th>\n",
       "      <th>uralic</th>\n",
       "      <th>urgic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aalborg</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aard</th>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>100.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          albanian    armenic  austronesian  bantu     celtic     chadic  \\\n",
       "aalborg  50.000000  50.000000     50.000000   50.0  50.000000  50.000000   \n",
       "aard     44.444444  44.444444     94.444444  100.0  44.444444  44.444444   \n",
       "\n",
       "          east_asia   germanic   hellenic  hindustani     italic  \\\n",
       "aalborg  100.000000  50.000000  50.000000   50.000000  50.000000   \n",
       "aard      88.888889  44.444444  44.444444   44.444444  44.444444   \n",
       "\n",
       "         native_american  native_cana    semitic     slavic     turkic  \\\n",
       "aalborg        50.000000    50.000000  50.000000  50.000000  50.000000   \n",
       "aard           44.444444    44.444444  44.444444  44.444444  44.444444   \n",
       "\n",
       "            uralic      urgic  \n",
       "aalborg  50.000000  50.000000  \n",
       "aard     44.444444  44.444444  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langfam_lemmapercentile = pd.read_csv(save_progress+\"langfam_lemmapercentile.csv\",index_col=0)\n",
    "langfam_lemmapercentile.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "650d4548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come\n",
      "take\n",
      "would\n",
      "king\n",
      "little\n",
      "give\n",
      "make\n",
      "could\n",
      "time\n",
      "look\n",
      "prince\n",
      "well\n",
      "back\n",
      "like\n",
      "great\n",
      "woman\n",
      "know\n",
      "away\n",
      "must\n",
      "young\n",
      "home\n",
      "princess\n",
      "never\n",
      "long\n",
      "brother\n",
      "house\n",
      "upon\n",
      "good\n",
      "shall\n",
      "mother\n",
      "thought\n",
      "told\n",
      "head\n",
      "hand\n",
      "night\n",
      "wife\n",
      "answer\n",
      "tree\n",
      "three\n",
      "child\n",
      "place\n",
      "water\n",
      "father\n",
      "daughter\n",
      "found\n",
      "call\n",
      "last\n",
      "soon\n",
      "heard\n",
      "begin\n",
      "even\n",
      "girl\n",
      "tell\n",
      "turn\n",
      "first\n",
      "people\n",
      "much\n",
      "till\n",
      "beautiful\n",
      "life\n",
      "thing\n",
      "palace\n",
      "want\n",
      "nothing\n",
      "live\n",
      "still\n",
      "find\n",
      "horse\n",
      "every\n",
      "bird\n",
      "reply\n",
      "return\n",
      "another\n",
      "poor\n",
      "might\n",
      "think\n",
      "walk\n",
      "many\n",
      "left\n",
      "knew\n",
      "carry\n",
      "sister\n",
      "word\n",
      "kill\n",
      "queen\n",
      "ever\n",
      "become\n",
      "stone\n",
      "reach\n",
      "fire\n",
      "mountain\n",
      "foot\n",
      "follow\n",
      "stood\n",
      "grow\n",
      "fairy\n",
      "without\n",
      "world\n",
      "giant\n",
      "brought\n",
      "wish\n",
      "always\n",
      "round\n",
      "help\n",
      "forest\n",
      "heart\n",
      "wood\n",
      "seem\n",
      "work\n",
      "moment\n",
      "door\n",
      "dragon\n",
      "quite\n",
      "room\n",
      "near\n",
      "husband\n",
      "together\n",
      "master\n",
      "order\n",
      "castle\n",
      "love\n",
      "maiden\n",
      "wait\n",
      "leave\n",
      "youth\n",
      "stop\n",
      "year\n",
      "land\n",
      "happen\n",
      "friend\n",
      "side\n",
      "next\n",
      "large\n",
      "lose\n",
      "white\n",
      "ground\n",
      "earth\n",
      "dead\n",
      "rest\n",
      "bring\n",
      "laugh\n",
      "right\n",
      "though\n",
      "face\n",
      "fell\n",
      "sleep\n",
      "open\n",
      "name\n",
      "also\n",
      "wolf\n",
      "behind\n",
      "enough\n",
      "however\n",
      "morning\n",
      "something\n",
      "show\n",
      "talk\n",
      "keep\n",
      "beast\n",
      "sent\n",
      "lead\n",
      "stand\n",
      "rise\n",
      "light\n",
      "food\n",
      "promise\n",
      "country\n",
      "step\n",
      "black\n",
      "death\n",
      "gold\n",
      "strong\n",
      "listen\n",
      "town\n",
      "pass\n",
      "everything\n",
      "full\n",
      "money\n",
      "monkey\n",
      "flower\n",
      "bear\n",
      "dress\n",
      "enter\n",
      "whole\n",
      "along\n",
      "dear\n",
      "hear\n",
      "voice\n",
      "fall\n",
      "among\n",
      "witch\n",
      "wind\n",
      "high\n",
      "river\n",
      "watch\n",
      "suddenly\n",
      "fill\n",
      "fine\n",
      "appear\n",
      "sword\n",
      "ready\n",
      "towards\n",
      "start\n",
      "fear\n",
      "able\n",
      "kept\n",
      "small\n",
      "garden\n",
      "thee\n",
      "part\n",
      "servant\n",
      "kind\n",
      "anything\n",
      "golden\n",
      "hold\n",
      "rich\n",
      "best\n",
      "dance\n",
      "second\n",
      "felt\n",
      "threw\n",
      "third\n",
      "lady\n",
      "fish\n",
      "happy\n",
      "wonder\n",
      "body\n",
      "beauty\n",
      "hair\n",
      "speak\n",
      "ring\n",
      "sight\n",
      "close\n",
      "sound\n",
      "rock\n",
      "around\n",
      "alone\n",
      "arrive\n",
      "afraid\n",
      "city\n",
      "indeed\n",
      "change\n",
      "remain\n",
      "leaf\n",
      "really\n",
      "play\n",
      "matter\n",
      "need\n",
      "thus\n",
      "thou\n",
      "window\n",
      "cover\n",
      "course\n",
      "longer\n",
      "wall\n",
      "sure\n",
      "flew\n",
      "strange\n",
      "dark\n",
      "hunt\n",
      "hill\n",
      "married\n",
      "pull\n",
      "tortoise\n",
      "present\n",
      "half\n",
      "story\n",
      "others\n",
      "often\n",
      "remember\n",
      "lord\n",
      "seize\n",
      "caught\n",
      "believe\n",
      "power\n",
      "field\n",
      "marry\n",
      "piece\n",
      "table\n",
      "receive\n",
      "since\n",
      "tire\n",
      "spoke\n",
      "stick\n",
      "green\n",
      "rush\n",
      "whose\n",
      "stay\n",
      "neither\n",
      "trouble\n",
      "quickly\n",
      "mind\n",
      "baby\n",
      "mean\n",
      "forth\n",
      "held\n",
      "save\n",
      "escape\n",
      "lion\n",
      "instead\n",
      "search\n",
      "fellow\n",
      "puma\n",
      "cross\n",
      "village\n",
      "hole\n",
      "straight\n",
      "hour\n",
      "hard\n",
      "already\n",
      "animal\n",
      "sultan\n",
      "drink\n",
      "tail\n",
      "immediately\n",
      "beside\n",
      "seek\n",
      "throw\n",
      "spring\n",
      "touch\n",
      "blood\n",
      "emperor\n",
      "robber\n",
      "kingdom\n",
      "blow\n",
      "wander\n",
      "travel\n",
      "drop\n",
      "free\n",
      "seven\n",
      "tear\n",
      "creature\n",
      "road\n",
      "rode\n",
      "almost\n",
      "laid\n",
      "notice\n",
      "feel\n",
      "silver\n",
      "struck\n",
      "church\n",
      "boat\n",
      "move\n",
      "across\n",
      "mouth\n",
      "inside\n",
      "hurry\n",
      "disappear\n",
      "heaven\n",
      "meet\n",
      "chief\n",
      "bride\n",
      "dream\n",
      "desire\n",
      "guard\n",
      "branch\n",
      "treasure\n",
      "moon\n",
      "angry\n",
      "snow\n",
      "shore\n",
      "lake\n",
      "soldier\n",
      "perhaps\n",
      "none\n",
      "four\n",
      "ship\n",
      "frighten\n",
      "pretty\n",
      "skin\n",
      "deep\n",
      "evil\n",
      "learn\n",
      "cave\n",
      "spirit\n",
      "finish\n",
      "length\n",
      "strength\n",
      "front\n",
      "fast\n",
      "outside\n",
      "island\n",
      "parent\n",
      "snake\n",
      "rabbit\n",
      "smile\n",
      "exclaim\n",
      "care\n",
      "next_morning\n",
      "clothes\n",
      "sing\n",
      "cloud\n",
      "catch\n",
      "wild\n",
      "stranger\n",
      "certainly\n",
      "break\n",
      "sheep\n",
      "journey\n",
      "bread\n",
      "cold\n",
      "cause\n",
      "knife\n",
      "fight\n",
      "roll\n",
      "question\n",
      "lift\n",
      "form\n",
      "star\n",
      "lovely\n",
      "therefore\n",
      "grass\n",
      "bright\n",
      "bound\n",
      "bush\n",
      "allow\n",
      "apple\n",
      "shepherd\n",
      "approach\n",
      "point\n",
      "noise\n",
      "send\n",
      "court\n",
      "fair\n",
      "enemy\n",
      "mouse\n",
      "milk\n",
      "heavy\n",
      "morrow\n",
      "long_time\n",
      "true\n",
      "wing\n",
      "broke\n",
      "dinner\n",
      "person\n",
      "fruit\n",
      "command\n",
      "offer\n",
      "merchant\n",
      "within\n",
      "agree\n",
      "spot\n",
      "drew\n",
      "hung\n",
      "wonderful\n",
      "bone\n",
      "hare\n",
      "finger\n",
      "thanked\n",
      "shoulder\n",
      "basket\n",
      "shout\n",
      "reward\n",
      "mine\n",
      "hero\n",
      "sprang\n",
      "family\n",
      "pleased\n",
      "wicked\n",
      "stream\n",
      "hall\n",
      "eagle\n",
      "please\n",
      "dare\n",
      "magic\n",
      "middle\n",
      "wave\n",
      "kiss\n",
      "hungry\n",
      "blue\n",
      "feast\n",
      "glad\n",
      "distance\n",
      "terrible\n",
      "mount\n",
      "hang\n",
      "farmer\n",
      "path\n",
      "visit\n",
      "sort\n",
      "wise\n",
      "beat\n",
      "meat\n",
      "stag\n",
      "mighty\n",
      "warm\n",
      "carefully\n",
      "jumped\n",
      "gate\n",
      "steal\n",
      "serpent\n",
      "whether\n",
      "knight\n",
      "shark\n",
      "maid\n",
      "sweet\n",
      "iron\n",
      "force\n",
      "refuse\n",
      "hope\n",
      "fetch\n",
      "stretch\n",
      "hardly\n",
      "cottage\n",
      "sometimes\n",
      "troll\n",
      "rather\n",
      "seat\n",
      "pain\n",
      "determine\n",
      "broken\n",
      "eldest\n",
      "either\n",
      "bade\n",
      "except\n",
      "asleep\n",
      "bury\n",
      "dawn\n",
      "ball\n",
      "slave\n",
      "weep\n",
      "hide\n",
      "nearly\n",
      "floor\n",
      "tiger\n",
      "although\n",
      "supper\n",
      "living\n",
      "declare\n",
      "several\n",
      "manner\n",
      "sooner\n",
      "company\n",
      "faithful\n",
      "slip\n",
      "fasten\n",
      "climb\n",
      "minute\n",
      "raise\n",
      "gather\n",
      "beggar\n",
      "plant\n",
      "month\n",
      "nose\n",
      "gain\n",
      "forget\n",
      "chance\n",
      "harm\n",
      "peasant\n",
      "corner\n",
      "drove\n",
      "wept\n",
      "past\n",
      "awoke\n",
      "delighted\n",
      "wedding\n",
      "sell\n",
      "feather\n",
      "soul\n",
      "continued\n",
      "winter\n",
      "serve\n",
      "consent\n",
      "plan\n",
      "suppose\n",
      "bottle\n",
      "empty\n",
      "arrow\n",
      "clear\n",
      "monster\n",
      "mare\n",
      "least\n",
      "song\n",
      "service\n",
      "flame\n",
      "frog\n",
      "companion\n",
      "shin\n",
      "pity\n",
      "tower\n",
      "expect\n",
      "royal\n",
      "built\n",
      "meadow\n",
      "decide\n",
      "danger\n",
      "manage\n",
      "pocket\n",
      "thousand\n",
      "sorrow\n",
      "bridge\n",
      "cast\n",
      "usual\n",
      "majesty\n",
      "huge\n",
      "meal\n",
      "wound\n",
      "whisper\n",
      "coat\n",
      "gift\n",
      "rope\n",
      "thief\n",
      "played\n",
      "fresh\n",
      "sack\n",
      "instantly\n",
      "peter\n",
      "brave\n",
      "crown\n",
      "handsome\n",
      "charm\n",
      "certain\n",
      "devil\n",
      "wine\n",
      "horn\n",
      "sail\n",
      "ugly\n",
      "summer\n",
      "hundred\n",
      "guest\n",
      "prepared\n",
      "direction\n",
      "neck\n",
      "nine\n",
      "secret\n",
      "fate\n",
      "shoe\n",
      "vain\n",
      "slept\n",
      "twelve\n",
      "magician\n",
      "scream\n",
      "yellow\n",
      "presently\n",
      "reason\n",
      "cord\n",
      "shut\n",
      "picked\n",
      "elephant\n",
      "primrose\n",
      "pray\n",
      "yust\n",
      "drive\n",
      "storm\n",
      "jewel\n",
      "invite\n",
      "bottom\n",
      "nice\n",
      "unless\n",
      "discover\n",
      "gazelle\n",
      "five\n",
      "spell\n",
      "swallow\n",
      "return_home\n",
      "easy\n",
      "rage\n",
      "hasten\n",
      "sang\n",
      "easily\n",
      "courage\n",
      "write\n",
      "grave\n",
      "thick\n",
      "born\n",
      "hearing\n",
      "human\n",
      "beneath\n",
      "wear\n",
      "besides\n",
      "everyone\n",
      "book\n",
      "grown\n",
      "splendid\n",
      "knock\n",
      "succeed\n",
      "suffer\n",
      "else\n",
      "alive\n",
      "different\n",
      "take_care\n",
      "truth\n",
      "push\n",
      "goat\n",
      "could_find\n",
      "honour\n",
      "welcome\n",
      "happiness\n",
      "spread\n",
      "priest\n",
      "roof\n",
      "depart\n",
      "clean\n",
      "forgot\n",
      "hidden\n",
      "safe\n",
      "case\n",
      "whatever\n",
      "street\n",
      "burn\n",
      "shook\n",
      "strike\n",
      "fool\n",
      "figure\n",
      "roar\n",
      "pearl\n",
      "number\n",
      "thank\n",
      "grief\n",
      "surely\n",
      "music\n",
      "marriage\n",
      "proud\n",
      "short\n",
      "neighbour\n",
      "quiet\n",
      "soft\n",
      "stroke\n",
      "surprise\n",
      "mistress\n",
      "crept\n",
      "terror\n",
      "task\n",
      "rain\n",
      "tiny\n",
      "darkness\n",
      "plain\n",
      "nobody\n",
      "flung\n",
      "blade\n",
      "beyond\n",
      "sigurd\n",
      "pleasure\n",
      "fail\n",
      "open_door\n",
      "chamber\n",
      "sick\n",
      "ought\n",
      "climbed\n",
      "lock\n",
      "advice\n",
      "relya\n",
      "crowd\n",
      "business\n",
      "loud\n",
      "wash\n",
      "news\n",
      "shoot\n",
      "forward\n",
      "jack\n",
      "pick\n",
      "cake\n",
      "kitchen\n",
      "bell\n",
      "pretend\n",
      "inquire\n",
      "anyone\n",
      "ride\n",
      "single\n",
      "tongue\n",
      "plunk\n",
      "rice\n",
      "midst\n",
      "blind\n",
      "folk\n",
      "father_mother\n",
      "noble\n",
      "forgotten\n",
      "rejoice\n",
      "idea\n",
      "flee\n",
      "late\n",
      "spear\n",
      "pipe\n",
      "tremble\n",
      "root\n",
      "peace\n",
      "cook\n",
      "chain\n",
      "army\n",
      "instant\n",
      "yonder\n",
      "drag\n",
      "draw\n",
      "plenty\n",
      "toward\n",
      "heal\n",
      "dwarf\n",
      "deliver\n",
      "luck\n",
      "midnight\n",
      "taste\n",
      "breath\n",
      "silent\n",
      "yard\n",
      "finally\n",
      "clever\n",
      "count\n",
      "warrior\n",
      "cure\n",
      "meanwhile\n",
      "repeat\n",
      "grandmother\n",
      "understand\n",
      "bank\n",
      "crow\n",
      "mile\n",
      "garment\n",
      "sign\n",
      "glass\n",
      "tale\n",
      "stayed\n",
      "gaze\n",
      "wide\n",
      "miss\n",
      "stuck\n",
      "anger\n",
      "vanished\n",
      "foolish\n",
      "silk\n",
      "adventure\n",
      "feed\n",
      "tall\n",
      "week\n",
      "shape\n",
      "stable\n",
      "warn\n",
      "honey\n",
      "scarcely\n",
      "oblige\n",
      "drank\n",
      "dwell\n",
      "fact\n",
      "cruel\n",
      "jackalse\n",
      "angel\n",
      "quick\n",
      "rule\n",
      "badger\n",
      "minister\n",
      "state\n",
      "sigh\n",
      "game\n",
      "branch_tree\n",
      "sand\n",
      "dish\n",
      "whenever\n",
      "less\n",
      "carriage\n",
      "tsar\n",
      "shot\n",
      "belong\n",
      "enjoy\n",
      "berry\n",
      "condition\n",
      "exactly\n",
      "battle\n",
      "shake\n",
      "choose\n",
      "fond\n",
      "pool\n",
      "content\n",
      "despair\n",
      "real\n",
      "flock\n",
      "heat\n",
      "thrown\n",
      "hunter\n",
      "surround\n",
      "nest\n",
      "share\n",
      "everybody\n",
      "treat\n",
      "mark\n",
      "account\n",
      "fortune\n",
      "captain\n",
      "obtain\n",
      "twice\n",
      "scoff\n",
      "arose\n",
      "rosy\n",
      "steed\n",
      "throne\n",
      "sorry\n",
      "taught\n",
      "paid\n",
      "fisherman\n",
      "consider\n",
      "imagine\n",
      "hurt\n",
      "guess\n",
      "counsel\n",
      "meant\n",
      "float\n",
      "doubt\n",
      "wrong\n",
      "hunger\n",
      "cock\n",
      "behold\n",
      "bowl\n",
      "raven\n",
      "demand\n",
      "fountain\n",
      "line\n",
      "seven_year\n",
      "burning\n",
      "directly\n",
      "anxious\n",
      "slowly\n",
      "beak\n",
      "accompany\n",
      "stir\n",
      "join\n",
      "comrade\n",
      "drown\n",
      "brown\n",
      "leap\n",
      "quietly\n",
      "farther\n",
      "desert\n",
      "swim\n",
      "lamb\n",
      "grey\n",
      "read\n",
      "judge\n",
      "beheld\n",
      "unhappy\n",
      "observe\n",
      "dere\n",
      "possible\n",
      "cage\n",
      "attendant\n",
      "glance\n",
      "teeth\n",
      "farewell\n",
      "letter\n",
      "presence\n",
      "cloak\n",
      "sought\n",
      "pair\n",
      "chase\n",
      "cease\n",
      "stole\n",
      "perceive\n",
      "cloth\n",
      "deer\n",
      "corn\n",
      "busy\n",
      "knee\n",
      "edge\n",
      "farm\n",
      "powerful\n",
      "summon\n",
      "nightingale\n",
      "settle\n",
      "howl\n",
      "gentleman\n",
      "courtyard\n",
      "appearance\n",
      "holy\n",
      "deed\n",
      "fault\n",
      "recover\n",
      "lodge\n",
      "thor\n",
      "chest\n",
      "lavender\n",
      "peach\n",
      "thou_hast\n",
      "lamp\n",
      "wore\n",
      "bore\n",
      "dame\n",
      "colour\n",
      "afterwards\n",
      "bought\n",
      "later\n",
      "spite\n",
      "jump\n",
      "load\n",
      "stepmother\n",
      "prayed\n",
      "sharp\n",
      "thereupon\n",
      "admire\n",
      "john\n",
      "picture\n",
      "opening\n",
      "scold\n",
      "march\n",
      "pour\n",
      "struggle\n",
      "trick\n",
      "loudly\n",
      "speed\n",
      "amuse\n",
      "resolve\n",
      "breast\n",
      "grant\n",
      "cattle\n",
      "grain\n",
      "devour\n",
      "turn_round\n",
      "bring_back\n",
      "bless\n",
      "kubik\n",
      "diamond\n",
      "surprised\n",
      "attend\n",
      "handkerchief\n",
      "fell_asleep\n",
      "comfort\n",
      "shone\n",
      "many_year\n",
      "early\n",
      "oven\n",
      "joyfully\n",
      "thunder\n",
      "bargain\n",
      "teach\n",
      "lonely\n",
      "dreadful\n",
      "cheek\n",
      "heap\n",
      "subject\n",
      "robe\n",
      "prove\n",
      "kick\n",
      "worth\n",
      "safely\n",
      "orange\n",
      "spoken\n",
      "woke\n",
      "greet\n",
      "softly\n",
      "purpose\n",
      "astonish\n",
      "famous\n",
      "whistle\n",
      "miller\n",
      "forty\n",
      "greatly\n",
      "beard\n",
      "flash\n",
      "swiftly\n",
      "punish\n",
      "pitcher\n",
      "weary\n",
      "suyettar\n",
      "rubbed\n",
      "thanks\n",
      "everywhere\n",
      "neighbor\n",
      "smell\n",
      "thin\n",
      "twenty\n",
      "spin\n",
      "bough\n",
      "bent\n",
      "mirror\n",
      "passing\n",
      "breakfast\n",
      "kindness\n",
      "bundle\n",
      "wherever\n",
      "beating\n",
      "satisfied\n",
      "shriek\n",
      "grieve\n",
      "proceed\n",
      "smoke\n",
      "till_reach\n",
      "blew\n",
      "crack\n",
      "dervish\n",
      "fold\n",
      "astonishment\n",
      "flow\n",
      "burnt\n",
      "dwelt\n",
      "race\n",
      "barrel\n",
      "understood\n",
      "accordingly\n",
      "sank\n",
      "terrify\n",
      "fierce\n",
      "trust\n",
      "couple\n",
      "miserable\n",
      "praise\n",
      "grand\n",
      "prayer\n",
      "girdle\n",
      "remark\n",
      "cradle\n",
      "coachman\n",
      "bark\n",
      "messenger\n",
      "advise\n",
      "delight\n",
      "sprinkle\n",
      "distant\n",
      "whilst\n",
      "gold_silver\n",
      "careful\n",
      "whip\n",
      "spent\n",
      "restore\n",
      "curse\n",
      "prepare\n",
      "heed\n",
      "fought\n",
      "suit\n",
      "smith\n",
      "scatter\n",
      "request\n",
      "pigeon\n",
      "horror\n",
      "nearer\n",
      "thread\n",
      "bath\n",
      "chair\n",
      "embrace\n",
      "difficulty\n",
      "wealth\n",
      "press\n",
      "tore\n",
      "nurse\n",
      "brush\n",
      "vessel\n",
      "sparrow\n",
      "silence\n",
      "fancy\n",
      "loose\n",
      "cavern\n",
      "ocean\n",
      "wake\n",
      "pale\n",
      "contain\n",
      "rescue\n",
      "collect\n",
      "sudden\n",
      "wrap\n",
      "thumb\n",
      "meantime\n",
      "ruin\n",
      "cunning\n",
      "bite\n",
      "shadow\n",
      "blossom\n",
      "tribe\n",
      "latter\n",
      "stair\n",
      "attempt\n",
      "dust\n",
      "conduct\n",
      "cellar\n",
      "short_time\n",
      "band\n",
      "loss\n",
      "explain\n",
      "publish\n",
      "building\n",
      "shirt\n",
      "amongst\n",
      "unknown\n",
      "duke\n",
      "whale\n",
      "flesh\n",
      "turnip\n",
      "shell\n",
      "unable\n",
      "valley\n",
      "enchant\n",
      "north\n",
      "grandfather\n",
      "note\n",
      "tiidu\n",
      "merry\n",
      "sadly\n",
      "gentle\n",
      "cliff\n",
      "duty\n",
      "make_mind\n",
      "elder\n",
      "mortal\n",
      "awake\n",
      "shelter\n",
      "token\n",
      "slipper\n",
      "persuade\n",
      "toad\n",
      "stork\n",
      "worm\n",
      "pile\n",
      "honor\n",
      "prison\n",
      "wisdom\n",
      "courtier\n",
      "fast_could\n",
      "impossible\n",
      "size\n",
      "right_hand\n",
      "mercy\n",
      "stupid\n",
      "punishment\n",
      "swan\n",
      "inform\n",
      "build\n",
      "straw\n",
      "faint\n",
      "provide\n",
      "camp\n",
      "eldest_brother\n",
      "dive\n",
      "thrust\n",
      "burst\n",
      "post\n",
      "indian\n",
      "increase\n",
      "could_hardly\n",
      "intend\n",
      "odin\n",
      "saddle\n",
      "live_happily\n",
      "goody\n",
      "misfortune\n",
      "whoever\n",
      "duck\n",
      "occasion\n",
      "eight\n",
      "exchange\n",
      "former\n",
      "awaken\n",
      "plunge\n",
      "venture\n",
      "mill\n",
      "release\n",
      "blessing\n",
      "bull\n",
      "kettle\n",
      "wooden\n",
      "claw\n",
      "common\n",
      "accord\n",
      "willing\n",
      "dove\n",
      "guide\n",
      "east\n",
      "boil\n",
      "swing\n",
      "recognize\n",
      "lover\n",
      "spider\n",
      "club\n",
      "coach\n",
      "dart\n",
      "glitter\n",
      "gleam\n",
      "attack\n",
      "magnificent\n",
      "quarrel\n",
      "hastily\n",
      "fourth\n",
      "afternoon\n",
      "second_time\n",
      "pleasant\n",
      "traveller\n",
      "fashion\n",
      "deceive\n",
      "board\n",
      "plough\n",
      "copper\n",
      "lean\n",
      "kindly\n",
      "poison\n",
      "marko\n",
      "keejeepaa\n",
      "await\n",
      "market\n",
      "weak\n",
      "chatter\n",
      "steadily\n",
      "gently\n",
      "shame\n",
      "future\n",
      "parrot\n",
      "yesterday\n",
      "party\n",
      "entrance\n",
      "overcome\n",
      "groan\n",
      "address\n",
      "loaf\n",
      "cool\n",
      "recognise\n",
      "widow\n",
      "gardener\n",
      "threaten\n",
      "host\n",
      "complain\n",
      "broad\n",
      "flight\n",
      "property\n",
      "pole\n",
      "horrible\n",
      "birch\n",
      "pursue\n",
      "bridegroom\n",
      "fright\n",
      "aside\n",
      "scratch\n",
      "tablecloth\n",
      "prisoner\n",
      "charge\n",
      "kerttu\n",
      "nature\n",
      "elsa\n",
      "favour\n",
      "throat\n",
      "narrow\n",
      "comb\n",
      "stoop\n",
      "sake\n",
      "someone\n",
      "herd\n",
      "wretched\n",
      "string\n",
      "reign\n",
      "assemble\n",
      "staff\n",
      "loki\n",
      "nevertheless\n",
      "ointment\n",
      "apple_tree\n",
      "drawn\n",
      "hollow\n",
      "sculpat\n",
      "wheat\n",
      "angrily\n",
      "separate\n",
      "temple\n",
      "worthy\n",
      "obey\n",
      "respect\n",
      "object\n",
      "remove\n",
      "boot\n",
      "message\n",
      "ameer\n",
      "mourn\n",
      "wait_till\n",
      "chin\n",
      "drinking\n",
      "illustration\n",
      "meeting\n",
      "shook_head\n",
      "sore\n",
      "paint\n",
      "destroy\n",
      "slain\n",
      "hammer\n",
      "calf\n",
      "huntsman\n",
      "prevent\n",
      "cooked\n",
      "mosque\n",
      "amaze\n",
      "affair\n",
      "precious\n",
      "trunk\n",
      "tailor\n",
      "dash\n",
      "effect\n",
      "drunk\n",
      "chose\n",
      "anywhere\n",
      "third_time\n",
      "paper\n",
      "crawl\n",
      "wheel\n",
      "heel\n",
      "store\n",
      "descend\n",
      "invisible\n",
      "arrange\n",
      "altogether\n",
      "daddy\n",
      "temper\n",
      "fairy_tale\n",
      "great_deal\n",
      "report\n",
      "cost\n",
      "worn\n",
      "trace\n",
      "veil\n",
      "possession\n",
      "uncle\n",
      "brook\n",
      "shade\n",
      "perish\n",
      "price\n",
      "worry\n",
      "joke\n",
      "examine\n",
      "speech\n",
      "cart\n",
      "apartment\n",
      "crystal\n",
      "wand\n",
      "circle\n",
      "perch\n",
      "obeyed\n",
      "gladly\n",
      "toss\n",
      "dread\n",
      "difficult\n",
      "beaten\n",
      "alight\n",
      "proper\n",
      "plate\n",
      "knock_door\n",
      "passage\n",
      "christian\n",
      "jealous\n",
      "peer\n",
      "flood\n",
      "padishah\n",
      "heard_sound\n",
      "mount_horse\n",
      "color\n",
      "entirely\n",
      "require\n",
      "clearing\n",
      "perform\n",
      "sparkle\n",
      "tent\n",
      "beloved\n",
      "mass\n",
      "badly\n",
      "completely\n",
      "doctor\n",
      "curious\n",
      "limb\n",
      "smooth\n",
      "mist\n",
      "especially\n",
      "bare\n",
      "ancient\n",
      "stall\n",
      "unfortunate\n",
      "slay\n",
      "ripe\n",
      "conquer\n",
      "quest\n",
      "seal\n",
      "grows\n",
      "tone\n",
      "weather\n",
      "dost_thou\n",
      "accept\n",
      "rose\n",
      "weight\n",
      "spend_night\n",
      "hurl\n",
      "precious_stone\n",
      "nail\n",
      "rough\n",
      "match\n",
      "bearing\n",
      "golden_hair\n",
      "embroider\n",
      "mock\n",
      "thou_wilt\n",
      "bidding\n",
      "barber\n",
      "marble\n",
      "curiosity\n",
      "capital\n",
      "quarter\n",
      "eagerly\n",
      "blast\n",
      "asgard\n",
      "pluck\n",
      "student\n",
      "coffin\n",
      "yell\n",
      "amazement\n",
      "weapon\n",
      "hedgehog\n",
      "foam\n",
      "rudy\n",
      "deserve\n",
      "sense\n",
      "conceal\n",
      "hast_thou\n",
      "befallen\n",
      "innocent\n"
     ]
    }
   ],
   "source": [
    "for i in nation_lemmamean_save.sort_values('common',ascending=False).head(1500).index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434baf26",
   "metadata": {},
   "source": [
    "# LIWC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "3b48e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict[0]['power'] = ['king','queen','prince','princess','servant','master','lord','throne']\n",
    "liwc_dict[0]['mythical'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "581f3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = utils.readDict(data+'LIWC2007_English100131.dic')\n",
    "# stories_liwc = pd.DataFrame()\n",
    "# for i in stories.index:\n",
    "#     text = stories['text'][i]\n",
    "#     liwc_counts = utils.wordCount(text, liwc_dict)\n",
    "#     liwc_df = pd.DataFrame(list(liwc_counts[0].items())).set_index(0) / liwc_counts[2]\n",
    "#     liwc_df.columns = [stories.loc[i,'sid']]\n",
    "#     stories_liwc = pd.concat([stories_liwc,liwc_df],axis=1)\n",
    "# stories_liwc = stories_liwc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9badef",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict[0]['power'] = ['king','queen','prince','princess','servant','master','lord','throne']\n",
    "for i in stories.index:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "2bed07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_liwc = pd.merge(stories_hofstede,stories_liwc,right_index=True,left_on='sid').set_index('sid')\n",
    "nation_liwc = nation_liwc.groupby('nation').mean().reset_index()\n",
    "nation_liwc.columns\n",
    "hofstede_cols = ['pdi','idv','mas','uai','ltowvs','ivr']\n",
    "liwc_cols = nation_liwc.columns[8:]\n",
    "for col in liwc_cols:\n",
    "    nation_liwc[col] = (nation_liwc[col] - nation_liwc[col].mean())/nation_liwc[col].std()\n",
    "\n",
    "nation_liwc.to_csv(save_progress+\"nation_liwc.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b559d",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "731b7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_corpus = [models.doc2vec.TaggedDocument(words=l, tags=[str(i)]) for i, l in enumerate(lemmas)]\n",
    "# doc_model = models.doc2vec.Doc2Vec(doc_corpus, min_count=200, window=10, workers=8)\n",
    "# doc_model.save(save_progress+'doc2vec.model')\n",
    "doc_model = models.doc2vec.Doc2Vec.load(save_progress+'doc2vec.model')\n",
    "# tsne = utils.doc2vec_tsne(doc_model, stories['language_family'])\n",
    "# tsne = pd.merge(tsne, stories[['nation','language_family']], left_index=True, right_index=True, how='left')\n",
    "# tsne = tsne.drop(index=tsne.query('y < -100').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "23298dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5112/4275761569.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec_tsne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language_family'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\macs_404_patterns\\folktales\\utils.py\u001b[0m in \u001b[0;36mdoc2vec_tsne\u001b[1;34m(doc_model, labels, colorby, perplexity, n_iter, n_components)\u001b[0m\n\u001b[0;32m    251\u001b[0m     tsne_model = TSNE(perplexity=perplexity, n_components=2, init='pca',\n\u001b[0;32m    252\u001b[0m                       n_iter=n_iter, random_state=23)\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \"\"\"\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mdegrees_of_freedom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         return self._tsne(P, degrees_of_freedom, n_samples,\n\u001b[0m\u001b[0;32m    842\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_iter_without_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0m\u001b[0;32m    898\u001b[0m                                                           **opt_args)\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compute_error'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[0m\u001b[0;32m    264\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tsne = utils.doc2vec_tsne(doc_model, stories['language_family'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f996109",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceddd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(lemmas.sample(frac=0.5))\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in lemmas]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "920ab7bd2fb7bd74b8de78632b6a92fbd66bc8d5dac803b94bc0f9d7f61a3b48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
